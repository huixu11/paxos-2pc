

import sys
import time
import csv
import random
import threading
import os
import sqlite3
from da import *  # DistAlgo primitives

# Provide a timeout helper when DA runtime does not supply one in run loop
def timeout(t: float):
	time.sleep(t)
	return True

# === Configuration ===
TIMEOUT = 5.0            # Base client/driver timeout (increased to ensure completion during failover)
TWO_PC_TIMEOUT = 4.5     # Internal 2PC wait (must be < client TIMEOUT)
HEARTBEAT_INTERVAL = 0.3
HEARTBEAT_TIMEOUT = 0.8
ELECTION_TIMEOUT = 1.2   # Faster failover to fit client timeout
PREPARE_BACKOFF = 0.3
PREPARE_RETRY_INTERVAL = 0.15  # More aggressive retries
MAX_CANDIDATE_TIME = 2 * ELECTION_TIMEOUT
ELECTION_JITTER = 0.3

VERBOSE_LOG = False
INITIAL_BALANCE = 10


# === Logging Helpers ===
def _emit(msg:str):
	try:
		output(msg)
	except Exception:
		print(msg)


def log_debug(msg:str):
	if VERBOSE_LOG:
		_emit(msg)


def log_info(msg:str):
	_emit(msg)


# === Helpers ===
def parse_transaction(tx_str):
	if not tx_str or tx_str.strip() == "":
		return None
	# Ignore failure/recovery markers that live in the tx column
	if tx_str.strip().startswith('F(') or tx_str.strip().startswith('R('):
		return None
	tx_str = tx_str.strip().strip('()')
	parts = [p.strip() for p in tx_str.split(',')]
	# Support for consistency level as 4th parameter
	# Format: (src, dst, amt, consistency) or (src, dst, amt)
	if len(parts) == 4:
		try:
			consistency = parts[3].strip().strip("'\"").lower()
			if consistency not in ('linearizable', 'eventual', 'read-your-writes', 'ryw'):
				consistency = 'linearizable'
			if consistency == 'ryw':
				consistency = 'read-your-writes'
			return (int(parts[0]), int(parts[1]), int(parts[2]), consistency)
		except Exception:
			return None
	if len(parts) == 3:
		try:
			return (int(parts[0]), int(parts[1]), int(parts[2]), 'linearizable')
		except Exception:
			return None
	# Balance query with optional consistency level
	if len(parts) == 2:
		try:
			consistency = parts[1].strip().strip("'\"").lower()
			if consistency not in ('linearizable', 'eventual', 'read-your-writes', 'ryw'):
				consistency = 'linearizable'
			if consistency == 'ryw':
				consistency = 'read-your-writes'
			return (int(parts[0]), consistency)
		except Exception:
			return None
	if len(parts) == 1:
		try:
			return (int(parts[0]), 'linearizable')
		except Exception:
			return None
	return None


def parse_live_nodes(nodes_str):
	if not nodes_str or nodes_str.strip() == "":
		return []
	nodes_str = nodes_str.strip().strip('[]')
	return [n.strip() for n in nodes_str.split(',') if n.strip()]


# === Configurable Cluster Settings ===
# Global configuration (can be overridden by CLI args)
CLUSTER_CONFIG = {
	'num_clusters': 3,
	'nodes_per_cluster': 3,
	'total_keys': 9000,
	'shard_map': {1: (1, 3000), 2: (3001, 6000), 3: (6001, 9000)}
}

#
# Dynamic shard overrides (updated by Reshard, applied offline between transactions).
# Each process has its own copy; Driver broadcasts updates via UPDATE_SHARD_OVERRIDES.
#
SHARD_OVERRIDES = {}

def generate_shard_map(num_clusters:int, total_keys:int = 9000):
	"""Generate shard map for given number of clusters."""
	keys_per_shard = total_keys // num_clusters
	shard_map = {}
	for i in range(1, num_clusters + 1):
		lo = (i - 1) * keys_per_shard + 1
		hi = i * keys_per_shard if i < num_clusters else total_keys
		shard_map[i] = (lo, hi)
	return shard_map

def generate_nodes(num_clusters:int, nodes_per_cluster:int):
	"""Generate node names for given configuration."""
	nodes = []
	node_idx = 1
	for _ in range(num_clusters * nodes_per_cluster):
		nodes.append(f'n{node_idx}')
		node_idx += 1
	return nodes

def _base_shard_for(key:int):
	"""Shard ID under the initial static range partitioning (ignores overrides)."""
	for shard_id, (lo, hi) in CLUSTER_CONFIG['shard_map'].items():
		if lo <= key <= hi:
			return shard_id
	# Fallback: use modular assignment
	return ((key - 1) % CLUSTER_CONFIG['num_clusters']) + 1

def shard_for(key:int):
	"""Get shard ID for a key, honoring dynamic overrides when present."""
	ov = SHARD_OVERRIDES.get(key)
	if ov:
		return ov
	return _base_shard_for(key)


def shard_bounds(shard_id:int):
	"""Get key range for a shard using configured shard map."""
	# Fallback uses configured total_keys for safety under custom cluster counts.
	total = CLUSTER_CONFIG.get('total_keys', 9000)
	keys_per = max(1, total // max(1, CLUSTER_CONFIG.get('num_clusters', 3)))
	fallback = (1, keys_per)
	return CLUSTER_CONFIG['shard_map'].get(shard_id, fallback)


class PersistentStore:
	"""SQLite-backed key/balance store with a dict-like surface."""

	def __init__(self, node_name:str, shard_id:int, base_dir:str = "data"):
		self.node_name = node_name
		self.shard_id = shard_id
		self.base_dir = base_dir
		os.makedirs(base_dir, exist_ok=True)
		self.path = os.path.join(base_dir, f"{node_name}_shard{shard_id}.sqlite")
		self.conn = sqlite3.connect(self.path, check_same_thread=False)
		self.conn.execute("PRAGMA journal_mode=WAL")
		self.conn.execute("CREATE TABLE IF NOT EXISTS accounts (key INTEGER PRIMARY KEY, balance INTEGER)")
		self.conn.commit()
		self._preload_if_empty()

	def _preload_if_empty(self):
		cur = self.conn.execute("SELECT COUNT(*) FROM accounts")
		row = cur.fetchone()
		if not row or row[0] == 0:
			start, end = shard_bounds(self.shard_id)
			rows = [(k, INITIAL_BALANCE) for k in range(start, end + 1)]
			self.conn.executemany("INSERT INTO accounts(key, balance) VALUES (?, ?)", rows)
			self.conn.commit()

	def reset(self):
		start, end = shard_bounds(self.shard_id)
		self.conn.execute("DELETE FROM accounts")
		rows = [(k, INITIAL_BALANCE) for k in range(start, end + 1)]
		self.conn.executemany("INSERT INTO accounts(key, balance) VALUES (?, ?)", rows)
		self.conn.commit()

	def _upsert_default(self, key:int, value:int):
		self.conn.execute("INSERT OR REPLACE INTO accounts(key, balance) VALUES (?, ?)", (key, value))
		self.conn.commit()
		return value

	def get(self, key:int, default=None):
		cur = self.conn.execute("SELECT balance FROM accounts WHERE key=?", (key,))
		row = cur.fetchone()
		if row is None:
			return default
		return row[0]

	def __getitem__(self, key:int):
		val = self.get(key, INITIAL_BALANCE)
		return self._upsert_default(key, val)

	def __setitem__(self, key:int, value:int):
		self._upsert_default(key, value)

	def __contains__(self, key:int):
		cur = self.conn.execute("SELECT 1 FROM accounts WHERE key=? LIMIT 1", (key,))
		return cur.fetchone() is not None

	def items(self):
		return [(row[0], row[1]) for row in self.conn.execute("SELECT key, balance FROM accounts")]

	def keys(self):
		return [row[0] for row in self.conn.execute("SELECT key FROM accounts")]

	def values(self):
		return [row[0] for row in self.conn.execute("SELECT balance FROM accounts")]


# === Parsing ===
def read_tests(csv_file):
	test_sets = []
	current = None
	with open(csv_file, 'r') as f:
		reader = csv.reader(f)
		next(reader, None)
		for row in reader:
			if not row or len(row) < 2:
				continue
			set_num = row[0].strip()
			tx_str = row[1].strip()
			live_nodes_str = row[2].strip() if len(row) > 2 else ''

			if set_num:
				if current:
					test_sets.append(current)
				current = {'id': set_num, 'transactions': [], 'live_nodes': parse_live_nodes(live_nodes_str)}

			if not current:
				continue

			if tx_str:
				if tx_str.startswith('F(') and tx_str.endswith(')'):
					current['transactions'].append(('FAIL', tx_str[2:-1]))
				elif tx_str.startswith('R(') and tx_str.endswith(')'):
					current['transactions'].append(('RECOVER', tx_str[2:-1]))
				else:
					tx = parse_transaction(tx_str)
					if tx:
						current['transactions'].append(('TX', tx))

		if current:
			test_sets.append(current)
	return test_sets


# === Driver ===

class Driver(process):
	def setup(csv_path:str, node_map:dict, clusters:dict, cluster_leaders:dict):
		self.test_sets = read_tests(csv_path)
		self.node_map = node_map
		self.clusters = clusters
		self.cluster_leaders = cluster_leaders
		self.proc_to_name = {proc: name for name, proc in node_map.items()}
		self.cluster_name_map = {cid: {self.proc_to_name.get(p, str(p)) for p in procs} for cid, procs in clusters.items()}
		self.pending = {}
		self.stats = {}
		self.balance_responses = {}
		self.all_nodes = set(node_map.values())
		# Prompts:
		# - In interactive terminal runs: default ON (per spec).
		# - When stdout is redirected (e.g., `> final.log 2>&1`): default OFF to avoid hanging on input().
		# Env overrides:
		# - PROMPT_SETS=1/0 forces set-to-set prompt on/off
		# - PROMPT_BALANCE=1/0 forces post-set command prompt on/off
		interactive = False
		try:
			interactive = bool(sys.stdin.isatty() and sys.stdout.isatty())
		except Exception:
			interactive = False

		def _env_flag(name: str, default: bool) -> bool:
			val = os.environ.get(name, None)
			if val is None:
				return default
			return val.lower() not in ('', '0', 'false', 'no')

		self.prompt_sets = _env_flag('PROMPT_SETS', interactive)
		self.prompt_balance = _env_flag('PROMPT_BALANCE', interactive)
		skip_env = os.environ.get('SKIP_SETS', '')
		self.skip_sets = {s.strip() for s in skip_env.split(',') if s.strip()}
		# Resharding: track cross-shard transaction pairs (src_key, dst_key)
		self.tx_history = []
		self.reshard_moves = []
		# Dynamic shard map (initially same as static)
		self.dynamic_shard_map = {}
		# Read-your-writes: track recent writes per key for RYW consistency
		# client_writes[key] = {'value': balance, 'ts': timestamp}
		self.client_writes = {}

	def run():
		for set_info in self.test_sets:
			set_id = set_info['id']
			txs = set_info['transactions']
			live_nodes = set(set_info['live_nodes']) if set_info['live_nodes'] else set(self.node_map.keys())

			# Allow skipping sets via env or optional prompt
			if str(set_id) in self.skip_sets:
				log_info(f"Skipping set {set_id} (SKIP_SETS)")
				continue
			if self.prompt_sets:
				try:
					ans = input(f"Run set {set_id}? [Enter=run / s=skip]: ").strip().lower()
				except EOFError:
					# Non-interactive: proceed rather than hanging.
					ans = ''
				except Exception:
					ans = ''
				if ans.startswith('s'):
					log_info(f"Skipping set {set_id} (user input)")
					continue

			# Flush state
			self.pending.clear()
			self.stats.clear()
			self.balance_responses.clear()
			# Spec: each set is an independent scenario; clear client-side history/state too.
			self.tx_history = []
			self.reshard_moves = []
			self.dynamic_shard_map = {}
			self.client_writes = {}
			# Spec: clear reshard mapping overrides between sets (Driver routing must reset too).
			SHARD_OVERRIDES.clear()
			# Best-effort: ensure all nodes also clear overrides even if a prior reshard broadcast raced.
			send(('UPDATE_SHARD_OVERRIDES', {}), to=list(self.all_nodes))
			send(('RESET',), to=list(self.all_nodes))
			time.sleep(TIMEOUT * 0.2)

			# Apply live set - send live node set to each node for accurate peer tracking
			live_procs_by_cluster = {}
			for name in live_nodes:
				proc = self.node_map.get(name)
				if proc:
					# Determine cluster from node name (n1-n3=c1, n4-n6=c2, n7-n9=c3 for 3x3)
					node_idx = int(name[1:])
					cluster_id = ((node_idx - 1) // CLUSTER_CONFIG['nodes_per_cluster']) + 1
					if cluster_id not in live_procs_by_cluster:
						live_procs_by_cluster[cluster_id] = set()
					live_procs_by_cluster[cluster_id].add(proc)
			
			for name, proc in self.node_map.items():
				if name in live_nodes:
					node_idx = int(name[1:])
					cluster_id = ((node_idx - 1) // CLUSTER_CONFIG['nodes_per_cluster']) + 1
					cluster_live_procs = live_procs_by_cluster.get(cluster_id, set())
					send(('RECOVER_WITH_PEERS', cluster_live_procs), to=proc)
				else:
					send(('FAIL',), to=proc)

			# Give enough time for Leader Election to finish (especially for Set 5 with failures)
			time.sleep(3.0)

			# Run transactions sequentially
			total = 0
			lat_sum = 0.0
			set_start = time.time()

			for idx, (kind, payload) in enumerate(txs, start=1):
				if kind == 'FAIL':
					if payload in self.node_map:
						send(('FAIL',), to=self.node_map[payload])
						live_nodes.discard(payload)
					continue
				if kind == 'RECOVER':
					if payload in self.node_map:
						send(('RECOVER',), to=self.node_map[payload])
						live_nodes.add(payload)
					continue
				if kind != 'TX':
					continue

				tx = payload
				txid = (int(set_id), idx)

				# Track transfer transactions for resharding analysis (hypergraph edges).
				# Support both (src,dst,amt) and (src,dst,amt,consistency)
				if len(tx) in (3, 4):
					src_key, dst_key = tx[0], tx[1]
					src_shard = shard_for(src_key)
					dst_shard = shard_for(dst_key)
					self.tx_history.append((src_key, dst_key, src_shard, dst_shard))
				
				# Handle read-your-writes for balance queries at the Driver level
				if len(tx) == 2 and isinstance(tx[1], str) and tx[1] in ('ryw', 'read-your-writes'):
					key = tx[0]
					# Check if we have a cached write for this key
					if key in self.client_writes:
						cached = self.client_writes[key]
						# Return cached value immediately
						lat_sum += 0.001  # minimal latency
						total += 1
						log_info(f"Set {set_id} tx {tx} -> RYW cache hit: balance={cached['value']}")
						continue

				start_ts = time.time()
				deadline = start_ts + TIMEOUT
				last_attempt = 0.0
				RETRY_INTERVAL = 0.8
				res = None
				
				# Get all potential targets in the shard
				shard = shard_for(tx[0]) if len(tx) >= 1 else 1
				shard_procs = list(self.clusters.get(shard, set()))
				# Shuffle for random load balancing/retries
				random.shuffle(shard_procs)
				target_idx = 0

				# Retry loop: resend CLIENT_TX until deadline or response
				while time.time() < deadline:
					if res is None and (time.time() - last_attempt >= RETRY_INTERVAL):
						# Strategy: Try different nodes in the shard if valid response not received
						leader_target = None
						
						# First attempt: Try to find a known live leader
						if last_attempt == 0.0:
							leader_proc = self.cluster_leaders.get(shard)
							leader_name = self.proc_to_name.get(leader_proc)
							if leader_name and leader_name in live_nodes:
								leader_target = leader_proc
						
						# Fallback/Retry: Pick next node from shuffled list (ignoring dead ones if possible, but trying all eventually)
						if not leader_target:
							# Filter candidates by live_nodes first
							live_candidates = [p for p in shard_procs if self.proc_to_name.get(p) in live_nodes]
							if live_candidates:
								# Rotate through live candidates
								leader_target = live_candidates[target_idx % len(live_candidates)]
								target_idx += 1
							else:
								# If no known live nodes, just try any node in shard (maybe live_nodes is stale)
								if shard_procs:
									leader_target = shard_procs[target_idx % len(shard_procs)]
									target_idx += 1
						
						if leader_target:
							send(('CLIENT_TX', tx, txid, self), to=leader_target)
						
						last_attempt = time.time()

					if txid in self.pending:
						res = self.pending.pop(txid)
						break
					await(timeout(0.05))

					if txid in self.pending:
						res = self.pending.pop(txid)
						break
					await(timeout(0.05))

				if res is None and txid in self.pending:
					res = self.pending.pop(txid)

				if res is not None:
					lat_sum += (time.time() - start_ts)
					total += 1
					log_info(f"Set {set_id} tx {tx} -> {res}")
				else:
					# System failed to respond even after retries -> likely fully partitioned or dead
					log_info(f"Set {set_id} tx {tx} timed out")

			elapsed = max(time.time() - set_start, 1e-6)
			throughput = total / elapsed
			avg_lat = (lat_sum / total) if total else 0.0
			log_info(f"Set {set_id} throughput={throughput:.2f} tx/s avg_lat={avg_lat:.3f}s count={total}")
			self.stats[set_id] = {'throughput': throughput, 'avg_lat': avg_lat, 'count': total}
			output(f"Set {set_id} throughput={throughput:.2f} avg_lat={avg_lat:.3f}s count={total}")

			# Post-set commands (PrintDB/PrintView/Performance/PrintBalance/Reshard) are handled in the prompt below.
			time.sleep(TIMEOUT * 0.1)

			if self.prompt_balance and not getattr(self, 'skip_next_set', False):
				while True:
					prompt = (
						f"Set {set_id} done. Commands: "
						f"PrintBalance(ID), PrintDB, PrintView, Performance, Reshard, "
						f"'next'/'n', 'skip', 'q'/'exit': "
					)
					output(prompt)
					cmd = ''
					try:
						cmd = input().strip()
					except EOFError:
						# Try to reattach to console on Windows
						try:
							import sys as _sys
							_fin = open('CONIN$', 'r')
							_sys.stdin = _fin
							cmd = input().strip()
						except Exception:
							output("PROMPT_BALANCE: no stdin available, skipping")
							cmd = ''
					if cmd.lower() in ('q', 'exit'):
						send(('STOP',), to=list(self.all_nodes))
						return
					if cmd.lower() == 'skip':
						self.skip_next_set = True
						break
					if cmd.lower() in ('next', 'n'):
						break
					if cmd.lower() in ('printdb', 'db'):
						send(('PRINT_DB', set_id), to=list(self.all_nodes))
						time.sleep(TIMEOUT * 0.1)
						continue
					if cmd.lower() in ('printview', 'view'):
						send(('PRINT_VIEW', set_id), to=list(self.all_nodes))
						time.sleep(TIMEOUT * 0.1)
						continue
					if cmd.lower() in ('performance', 'perf'):
						self._print_perf()
						continue
					if cmd.lower() == 'reshard':
						self._do_reshard()
						continue
					if not cmd:
						output("Please enter a command. Use 'next' or 'n' to continue.")
						continue
					to_run = []
					payload = cmd.replace('PrintBalance', '').replace('(', '').replace(')', '')
					for part in payload.split(','):
						p = part.strip()
						if p.isdigit():
							to_run.append(int(p))
					for key in to_run:
						self._print_balance(key)
			else:
				self.skip_next_set = False

		send(('STOP',), to=list(self.all_nodes))

	def receive(msg=('TX_RESULT', req_id, result)):
		self.pending[req_id] = result
		# Track writes for read-your-writes consistency
		if isinstance(result, tuple) and len(result) == 2:
			result_type, write_info = result
			if result_type == 'COMMIT' and isinstance(write_info, dict):
				# Update client_writes cache with committed key->balance
				for key, balance in write_info.items():
					self.client_writes[key] = {'value': balance, 'ts': time.time()}

	def receive(msg=('BALANCE_RESULT', req_id, node_name, bal)):
		bucket = self.balance_responses.get(req_id)
		if bucket is None:
			bucket = {}
			self.balance_responses[req_id] = bucket
		bucket[node_name] = bal

	def receive(msg=('PRINT_PERF',)):
		self._print_perf()

	def _print_perf():
		if not self.stats:
			output("Performance: []")
			return
		for sid in sorted(self.stats.keys()):
			stat = self.stats[sid]
			output(f"Set {sid} throughput={stat['throughput']:.2f} avg_lat={stat['avg_lat']:.3f}s count={stat['count']}")

	def receive(msg=('PRINT_BALANCE', key_id)):
		self._print_balance(key_id)

	def _print_balance(key_id:int):
		shard = shard_for(key_id)
		targets = list(self.clusters.get(shard, set()))
		if not targets:
			output(f"PrintBalance({key_id}): []")
			return
		req_id = (key_id, time.time())
		self.balance_responses[req_id] = {}
		send(('GET_BALANCE', key_id, req_id, self), to=targets)
		deadline = time.time() + TIMEOUT
		while time.time() < deadline:
			if len(self.balance_responses.get(req_id, {})) >= len(targets):
				break
			await(timeout(0.05))
		results = self.balance_responses.pop(req_id, {})
		ordered = [f"{name} : {results.get(name, '?')}" for name in sorted(results.keys())]
		if ordered:
			# Match spec example output: "n4 : 8, n5 : 8, n6 : 10"
			output(", ".join(ordered))
		else:
			missing = [self.proc_to_name.get(p, str(p)) for p in targets]
			output(f"missing replies from {missing}")

	def _do_reshard():
		"""Reshard according to the spec idea (transaction hypergraph partitioning, offline).

		We build a weighted hypergraph from the last n transfer transactions, then compute a
		balanced 3-way partition using a lightweight greedy local improvement heuristic.
		Finally we migrate moved keys and broadcast the updated shard overrides.
		"""
		if not self.tx_history:
			output("Reshard: []")
			return

		from collections import Counter, defaultdict

		# Use the last n transfer transactions as hyperedges (for transfers it's a 2-vertex hyperedge).
		n = 200
		history = self.tx_history[-n:]

		edge_w = Counter()
		adj = defaultdict(dict)  # adj[u][v] = weight
		keys = set()
		for src, dst, _ss, _ds in history:
			if src == dst:
				continue
			a, b = (src, dst) if src < dst else (dst, src)
			edge_w[(a, b)] += 1
			keys.add(src)
			keys.add(dst)

		if not edge_w:
			output("Reshard: []")
			return

		for (a, b), w in edge_w.items():
			adj[a][b] = adj[a].get(b, 0) + w
			adj[b][a] = adj[b].get(a, 0) + w

		# Current assignment + shard sizes (considering current overrides)
		assign = {k: shard_for(k) for k in keys}

		target = CLUSTER_CONFIG['total_keys'] // CLUSTER_CONFIG['num_clusters']
		slack = max(1, int(target * 0.05))  # 5% slack

		# Compute current sizes from base partition then apply overrides
		sizes = {}
		for sid, (lo, hi) in CLUSTER_CONFIG['shard_map'].items():
			sizes[sid] = (hi - lo + 1)
		for k, sid in SHARD_OVERRIDES.items():
			base = _base_shard_for(k)
			if base != sid:
				sizes[base] = sizes.get(base, 0) - 1
				sizes[sid] = sizes.get(sid, 0) + 1

		def score_in_shard(key: int, shard_id: int) -> int:
			"""Total affinity weight from key to neighbors currently in shard_id."""
			total = 0
			for nb, w in adj.get(key, {}).items():
				if assign.get(nb) == shard_id:
					total += w
			return total

		# Greedy local improvement: move a key if it increases intra-shard affinity and keeps balance.
		changed = True
		for _round in range(6):
			if not changed:
				break
			changed = False
			for k in list(keys):
				cur = assign[k]
				cur_score = score_in_shard(k, cur)
				best = cur
				best_gain = 0
				for sid in range(1, CLUSTER_CONFIG['num_clusters'] + 1):
					if sid == cur:
						continue
					# balance constraint
					if sizes.get(sid, 0) + 1 > target + slack:
						continue
					if sizes.get(cur, 0) - 1 < target - slack:
						continue
					new_score = score_in_shard(k, sid)
					gain = new_score - cur_score
					if gain > best_gain:
						best_gain = gain
						best = sid
				if best != cur:
					assign[k] = best
					sizes[cur] = sizes.get(cur, 0) - 1
					sizes[best] = sizes.get(best, 0) + 1
					changed = True

		# Build moves: keys whose shard changed
		moves = []
		used_keys = set()
		for k, new_sid in assign.items():
			old_sid = shard_for(k)
			if new_sid != old_sid:
				moves.append((k, f"c{old_sid}", f"c{new_sid}", old_sid, new_sid))
				used_keys.add(k)

		# Fallback: if the local-improvement heuristic found no move, try a small number of
		# high-impact cross-shard edges (still respecting balance). This avoids surprising "[]"
		# for workloads that clearly have many cross-shard transfers.
		if not moves:
			top_edges = edge_w.most_common(20)
			for (a, b), w in top_edges:
				sa = shard_for(a)
				sb = shard_for(b)
				if sa == sb:
					continue
				# Try moving one endpoint to the other's shard.
				# Prefer moving a key that keeps balance constraints satisfied.
				cands = [(a, sa, sb), (b, sb, sa)]
				best = None
				for key, from_sid, to_sid in cands:
					if key in used_keys:
						continue
					if sizes.get(to_sid, 0) + 1 > target + slack:
						continue
					if sizes.get(from_sid, 0) - 1 < target - slack:
						continue
					best = (key, from_sid, to_sid)
					break
				if best:
					key, from_sid, to_sid = best
					moves.append((key, f"c{from_sid}", f"c{to_sid}", from_sid, to_sid))
					used_keys.add(key)
					sizes[from_sid] = sizes.get(from_sid, 0) - 1
					sizes[to_sid] = sizes.get(to_sid, 0) + 1
				if len(moves) >= 5:
					break

		if not moves:
			output("Reshard: []")
			return

		# Apply moves offline: migrate data, update overrides, broadcast
		for k, _s, _d, old_sid, new_sid in moves:
			source_nodes = list(self.clusters.get(old_sid, set()))
			dest_nodes = list(self.clusters.get(new_sid, set()))
			if source_nodes and dest_nodes:
				send(('MIGRATE_KEY', k, new_sid, dest_nodes), to=source_nodes)
				SHARD_OVERRIDES[k] = new_sid

		# Broadcast new overrides so future routing uses the updated mapping.
		# IMPORTANT: send deltas in chunks to avoid DA MessageTooBigException.
		chunk = {}
		CHUNK_SIZE = 200
		for k, _s, _d, _old_sid, new_sid in moves:
			chunk[int(k)] = int(new_sid)
			if len(chunk) >= CHUNK_SIZE:
				send(('UPDATE_SHARD_OVERRIDES_DELTA', dict(chunk)), to=list(self.all_nodes))
				chunk = {}
		if chunk:
			send(('UPDATE_SHARD_OVERRIDES_DELTA', dict(chunk)), to=list(self.all_nodes))

		self.reshard_moves = [(k, s, d) for (k, s, d, _o, _n) in moves]
		time.sleep(0.3)

		triplets = ", ".join([f"({k}, {s}, {d})" for (k, s, d, _o, _n) in moves])
		output(f"Reshard: {triplets}")


# === Node ===

class Node(process):
	def setup(node_name:str, cluster_id:int, peers:set, all_nodes:set, cluster_map:dict, initial_leader:bool, initial_leader_proc, proc_to_name:dict):
		self.node_name = node_name
		self.cluster_id = cluster_id
		self.peers = peers - {self}
		# Fixed cluster size (do NOT shrink quorum based on failures); f=1 with 3 nodes => quorum=2 always.
		self.cluster_size = len(self.peers) + 1
		self.all_nodes = all_nodes
		self.cluster_map = {k: set(v) for k, v in cluster_map.items()}
		self.is_initial_leader = initial_leader
		self.initial_leader_proc = initial_leader_proc
		self.proc_to_name = proc_to_name  # Mapping from process to node name

		self.status = 'LEADER' if initial_leader else 'FOLLOWER'
		self.ballot_num = (1, node_name, self) if initial_leader else (0, node_name, self)
		self.leader = self if initial_leader else initial_leader_proc
		self.current_leader = self.leader
		self.accept_log = set()
		self.committed_log = {}
		self.accepted_votes = {}
		self.promises = {}
		self.next_seq = 1
		self.last_executed = 0

		self.last_leader_msg = time.time()
		self.buffered_prepare = None
		self.last_prepare_received_time = 0.0
		self.last_election_time = 0.0
		self.candidate_start_time = 0.0
		self.leader_grace_until = 0.0
		self.live_peers = set(self.peers)
		# live_peers tracks currently reachable peers; quorum is based on cluster_size.
		self.live_cluster_size = self.cluster_size
		self.modified_keys = set()

		self.db = PersistentStore(node_name, cluster_id)
		self.client_replies = {}
		self.pending_requests = set()
		self.sent_replies = set()
		self.locks = {}
		self.tx_table = {}
		self.tx_meta = {}
		self.wal = {}
		self.decision_outbox = {}
		self.pending_2pc = {}
		self.tx_client = {}
		self.backlog_prepared = set()
		self.backlog_decisions = []
		self.backlog_prepares = []
		# Track PREPARED messages that need retry (txid -> {coord_shard, last_sent, decision_received})
		self.pending_prepared_outbox = {}

		self.new_view_messages = []
		self.reshard_events = []
		if self.is_initial_leader:
			self.new_view_messages.append(('NEW-VIEW', self.ballot_num, len(self.accept_log)))

		self.is_failed = False
		self.shutdown = False

	# === Run loop ===
	def run():
		while not self.shutdown:
			if self.is_failed:
				await(self.is_failed == False or self.shutdown)
				continue

			if self.status == 'LEADER':
				# Process any buffered client requests now that we lead.
				for txid, tx, client in list(self.pending_requests):
					handle_client_tx(tx, txid, client)
					self.pending_requests.discard((txid, tx, client))
				if self.live_peers:
					send(('HEARTBEAT', self.ballot_num, self), to=self.live_peers)
				await(timeout(HEARTBEAT_INTERVAL * 0.75))
				# Resend outstanding 2PC decisions if needed
				now = time.time()
				for txid, info in list(self.decision_outbox.items()):
					if now - info.get('last_sent', 0) >= PREPARE_RETRY_INTERVAL:
						desc = info.get('decision')
						client_reply = info.get('client_reply', desc)
						targets = info.get('targets', set())
						if targets:
							send(('DECISION', txid, desc), to=targets)
						self.decision_outbox[txid]['last_sent'] = now
						client = info.get('client')
						if client:
							send(('TX_RESULT', txid, client_reply), to=client)
				# Drive any fully prepared-but-undecided 2PCs after leadership changes.
				drive_pending_decisions()
				# Also recover incomplete 2PCs (COORD prepared but PART not yet)
				drive_incomplete_2pc()
				for txid, meta in list(self.pending_2pc.items()):
					# Retry PREPARE_TX to participants still pending
					if meta.get('state') == 'WAIT_PREPARED':
						participants = meta.get('participants', set())
						ready = meta.get('participants_ready', set())
						for shard in participants:
							if shard in ready:
								continue
							last_sent = meta.get('last_prepare_sent', 0)
							if now - last_sent >= PREPARE_RETRY_INTERVAL:
								targets = self.cluster_map.get(shard, set())
								if targets:
									send(('PREPARE_TX', txid, self.tx_meta[txid]['tx'], self.cluster_id, meta.get('seq_p'), meta.get('seq_p')), to=targets)
									meta['last_prepare_sent'] = now
									self.pending_2pc[txid] = meta
					if now - meta.get('ts', now) > TWO_PC_TIMEOUT and txid in self.tx_meta:
						decision = 'ABORT'
						seq_c = self.tx_meta[txid].get('commit_seq') or self.tx_meta[txid].get('prepare_seq') or self.next_seq
						propose_with_seq(seq_c, ('2PC_DECISION', txid, decision, 'COORD'))
						for shard in self.tx_meta[txid].get('participants', ()): 
							targets = self.cluster_map.get(shard, set())
							if targets:
								send(('DECISION', txid, decision), to=targets)
								self.decision_outbox[txid] = {'decision': decision, 'targets': set(targets), 'last_sent': now, 'client': self.tx_meta[txid].get('client')}
						del self.pending_2pc[txid]
				# Retry pending PREPARED messages to coordinator
				for txid, info in list(self.pending_prepared_outbox.items()):
					if info.get('decision_received'):
						del self.pending_prepared_outbox[txid]
						continue
					# Stop retrying if transaction is too old (beyond 2PC timeout)
					# Use last_sent as fallback if entry_ts missing, or treat as expired
					entry_ts = info.get('entry_ts') or info.get('last_sent') or 0
					if entry_ts == 0 or (now - entry_ts > TWO_PC_TIMEOUT + 1.0):
						del self.pending_prepared_outbox[txid]
						continue
					# Limit max retries to prevent message flood
					retry_count = info.get('retry_count', 0)
					if retry_count >= 10:
						del self.pending_prepared_outbox[txid]
						continue
					if now - info.get('last_sent', 0) >= PREPARE_RETRY_INTERVAL:
						coord_shard = info.get('coord_shard')
						targets = self.cluster_map.get(coord_shard, set())
						if targets:
							send(('PREPARED', txid, info.get('tx'), coord_shard, info.get('part_write') or {}), to=targets)
							self.pending_prepared_outbox[txid]['last_sent'] = now
							self.pending_prepared_outbox[txid]['retry_count'] = retry_count + 1
			elif self.status == 'FOLLOWER':
				old = self.last_leader_msg
				jitter = random.uniform(0, ELECTION_JITTER)
				to_wait = (ELECTION_TIMEOUT + jitter) - (time.time() - old)
				if to_wait < 0:
					to_wait = 0
				if await(self.last_leader_msg > old):
					pass
				elif timeout(to_wait):
					handle_timer_expiry()
				# Clean up stale pending_prepared_outbox entries (FOLLOWERs shouldn't have active retries)
				now = time.time()
				for txid in list(self.pending_prepared_outbox.keys()):
					info = self.pending_prepared_outbox.get(txid, {})
					entry_ts = info.get('entry_ts') or info.get('last_sent') or 0
					if entry_ts == 0 or (now - entry_ts > TWO_PC_TIMEOUT + 1.0):
						del self.pending_prepared_outbox[txid]
				# Periodically replay backlog queues even as follower to avoid loss
				if self.backlog_prepares:
					pending_preps = list(self.backlog_prepares)
					self.backlog_prepares = []
					for txid, tx, coord_shard, seq_p, seq_c in pending_preps:
						send(('PREPARE_TX', txid, tx, coord_shard, seq_p, seq_c), to=self)
				if self.backlog_prepared:
					pending_prepared = list(self.backlog_prepared)
					self.backlog_prepared.clear()
					for txid in pending_prepared:
						meta = self.tx_meta.get(txid, {})
						tx = meta.get('tx')
						coord_shard = meta.get('coord_shard')
						send(('PREPARED', txid, tx, coord_shard), to=self)
				if self.backlog_decisions:
					pending_dec = list(self.backlog_decisions)
					self.backlog_decisions = []
					for txid, decision in pending_dec:
						send(('DECISION', txid, decision), to=self)
			else:  # CANDIDATE
				if await(self.status != 'CANDIDATE' or self.shutdown):
					pass
				elif timeout(PREPARE_RETRY_INTERVAL):
					now = time.time()
					if self.candidate_start_time and (now - self.candidate_start_time) >= MAX_CANDIDATE_TIME:
						self.status = 'FOLLOWER'
						self.last_leader_msg = now
					elif self.live_peers:
						send(('PREPARE', self.ballot_num), to=self.live_peers)
						self.last_leader_msg = now

	# === Heartbeats & Election ===
	def drive_pending_decisions():
		for txid, meta in list(self.tx_meta.items()):
			if not meta or meta.get('decision'):
				continue
			if meta.get('prepared', {}).get('COORD') and meta.get('prepared', {}).get('PART'):
					decision = 'COMMIT'
					seq_c = meta.get('commit_seq') or meta.get('prepare_seq') or self.next_seq
					if seq_c is None:
						seq_c = self.next_seq
					propose_with_seq(seq_c, ('2PC_DECISION', txid, decision, 'COORD'))
					client = meta.get('client')
					for shard in meta.get('participants', ()): 
						targets = self.cluster_map.get(shard, set())
						if targets:
							send(('DECISION', txid, decision), to=targets)
							reply = decision
							tx = meta.get('tx')
							if decision == 'COMMIT' and tx and len(tx) >= 3:
								src, dst, _amt = tx[0], tx[1], tx[2]
								write_info = {src: self.db.get(src, INITIAL_BALANCE)}
								pw = meta.get('part_write') if isinstance(meta.get('part_write'), dict) else {}
								for k, v in (pw or {}).items():
									write_info[int(k)] = v
								reply = ('COMMIT', write_info)
							self.decision_outbox[txid] = {'decision': decision, 'client_reply': reply, 'targets': set(targets), 'last_sent': time.time(), 'client': client}
					if client:
						send(('TX_RESULT', txid, reply), to=client)

	def drive_incomplete_2pc():
		"""Recover incomplete 2PC transactions when becoming new leader.
		Resend PREPARE_TX to participants that haven't responded yet."""
		now = time.time()
		for txid, meta in list(self.tx_meta.items()):
			if not meta or meta.get('decision'):
				continue
			# Only handle transactions where we are coordinator (COORD is prepared)
			if not meta.get('prepared', {}).get('COORD'):
				continue
			# If PART not yet prepared, resend PREPARE_TX
			if not meta.get('prepared', {}).get('PART'):
				tx = meta.get('tx')
				if not tx:
					continue
				participants = meta.get('participants', ())
				seq_p = meta.get('prepare_seq') or self.next_seq
				for shard in participants:
					targets = self.cluster_map.get(shard, set())
					if targets:
						log_debug(f"{self.node_name} drive_incomplete_2pc: resending PREPARE_TX txid={txid} to shard={shard}")
						send(('PREPARE_TX', txid, tx, self.cluster_id, seq_p, seq_p), to=targets)
				# Add to pending_2pc if not already there
				if txid not in self.pending_2pc:
					self.pending_2pc[txid] = {'state': 'WAIT_PREPARED', 'ts': now, 'seq_p': seq_p, 'participants': set(participants), 'participants_ready': set(), 'last_prepare_sent': now}

	def handle_timer_expiry():
		if self.status != 'FOLLOWER' or self.is_failed:
			return
		if (time.time() - self.last_leader_msg) < HEARTBEAT_TIMEOUT:
			return
		now = time.time()
		if (now - self.last_prepare_received_time) < PREPARE_BACKOFF:
			return
		start_election()

	def should_i_lead():
		live_names = [self.proc_to_name.get(p, str(p)) for p in self.live_peers] + [self.node_name]
		return self.node_name == min(live_names)

	def start_election():
		if self.is_failed or self.status == 'CANDIDATE' or self.status == 'LEADER':
			log_debug(f"{self.node_name} start_election: skipped (failed={self.is_failed}, status={self.status})")
			return
		# Allow any live node to start election.
		# Rationale: with stale membership views (e.g., after leader fail), restricting election
		# initiation to a single "min node" can deadlock progress and cause client timeouts.
		self.status = 'CANDIDATE'
		self.ballot_num = (self.ballot_num[0] + 1, self.node_name, self)
		key = (self.ballot_num[0], self.ballot_num[1])
		self.promises[key] = {self: set(self.accept_log)}
		self.last_election_time = time.time()
		self.candidate_start_time = time.time()
		self.last_leader_msg = time.time()
		log_debug(f"{self.node_name} start_election: becoming CANDIDATE, ballot={self.ballot_num[0]}, live_peers={len(self.live_peers)}")
		if not self.live_peers:
			# Single live node: self-elect immediately so progress continues.
			log_debug(f"{self.node_name} start_election: no peers, self-electing")
			become_leader()
			return
		if self.live_peers:
			log_debug(f"{self.node_name} start_election: sending PREPARE to {len(self.live_peers)} peers")
			send(('PREPARE', self.ballot_num), to=self.live_peers)

# Conservative helper: follow known leader; if none, trigger election
	def ensure_leader():
		if self.status == 'LEADER':
			return True
		# If current_leader is in live_peers, wait for it; otherwise start election
		if self.current_leader and self.current_leader != self and self.current_leader in self.live_peers:
			return False
		# No valid leader known, start election
		if self.status != 'CANDIDATE':
			log_debug(f"{self.node_name} ensure_leader: no valid leader, starting election")
			start_election()
		return False

	def become_leader():
		self.status = 'LEADER'
		self.leader = self
		self.current_leader = self
		self.leader_grace_until = time.time() + ELECTION_TIMEOUT * 2  # Extended grace period
		self.live_cluster_size = self.cluster_size
		key = (self.ballot_num[0], self.ballot_num[1])
		for acc_log in self.promises.get(key, {}).values():
			self.accept_log.update(acc_log)
		self.new_view_messages.append(('NEW-VIEW', self.ballot_num, len(self.accept_log)))
		log_debug(f"{self.node_name} become_leader: ballot={self.ballot_num[0]}, live_cluster_size={self.live_cluster_size}, backlog_prepares={len(self.backlog_prepares)}")
		if self.live_peers:
			send(('HEARTBEAT', self.ballot_num, self), to=self.live_peers)
        
		# Process any backlogged PREPARED/DECISION messages captured while leader was unknown
		if self.backlog_prepared:
			pending_prepared = list(self.backlog_prepared)
			self.backlog_prepared.clear()
			for txid in pending_prepared:
				meta = self.tx_meta.get(txid, {})
				tx = meta.get('tx')
				coord_shard = meta.get('coord_shard')
				send(('PREPARED', txid, tx, coord_shard), to=self)
		if self.backlog_decisions:
			pending_dec = list(self.backlog_decisions)
			self.backlog_decisions = []
			for txid, decision in pending_dec:
				send(('DECISION', txid, decision), to=self)
		if self.backlog_prepares:
			pending_preps = list(self.backlog_prepares)
			self.backlog_prepares = []
			log_debug(f"{self.node_name} become_leader: replaying {len(pending_preps)} backlog PREPARE_TX")
			for txid, tx, coord_shard, seq_p, seq_c in pending_preps:
				send(('PREPARE_TX', txid, tx, coord_shard, seq_p, seq_c), to=self)
		
		# Recover any incomplete 2PC transactions as new leader
		drive_incomplete_2pc()
		drive_pending_decisions()

	# === Paxos message handlers ===
	def receive(msg=('HEARTBEAT', b, leader_proc), from_=p):
		if self.is_failed:
			return
		if leader_proc == self.current_leader or b >= self.ballot_num:
			if b > self.ballot_num:
				self.ballot_num = b
			self.status = 'FOLLOWER'
			self.leader = leader_proc
			self.current_leader = leader_proc
			self.last_leader_msg = time.time()

	def receive(msg=('PREPARE', b), from_=p):
		if self.is_failed:
			return
		now = time.time()
		self.last_prepare_received_time = now
		# Extend grace period if we have active transactions
		active_2pc = len(self.pending_2pc) > 0 or len(self.decision_outbox) > 0
		grace_time = self.leader_grace_until if not active_2pc else now + ELECTION_TIMEOUT
		if self.status == 'LEADER' and now < grace_time:
			log_debug(f"{self.node_name} ignoring PREPARE from {self.proc_to_name.get(p, str(p))} during grace (active_2pc={active_2pc})")
			return
		if b >= self.ballot_num:
			accept_prepare(b, p)
			return
		if self.buffered_prepare is None or b > self.buffered_prepare[0]:
			self.buffered_prepare = (b, p)

	def accept_prepare(b, proposer):
		proposer_name = self.proc_to_name.get(proposer, str(proposer))
		log_debug(f"{self.node_name} accept_prepare: accepting ballot {b[0]} from {proposer_name}")
		self.ballot_num = b
		self.status = 'FOLLOWER'
		self.leader = proposer
		self.current_leader = proposer
		self.candidate_start_time = 0.0
		self.buffered_prepare = None
		self.last_leader_msg = time.time()
		serialized_log = []
		for (ballot, seq, val) in self.accept_log:
			ballot_key = (ballot[0], ballot[1]) if isinstance(ballot, tuple) and len(ballot) >= 2 else ballot
			serialized_log.append((ballot_key, seq, val))
		b_key = (b[0], b[1]) if isinstance(b, tuple) and len(b) >= 2 else b
		send(('ACK', b_key, serialized_log), to=proposer)

	def receive(msg=('ACK', b, acc_log), from_=p):
		b_key = (b[0], b[1]) if len(b) >= 2 else b
		my_key = (self.ballot_num[0], self.ballot_num[1]) if len(self.ballot_num) >= 2 else self.ballot_num
		if isinstance(acc_log, list):
			acc_log = set(acc_log)
		if self.is_failed:
			return
		peer_name = self.proc_to_name.get(p, str(p))
		log_debug(f"{self.node_name} receive(ACK) from {peer_name}: b_key={b_key}, my_key={my_key}, status={self.status}")
		if self.status != 'CANDIDATE':
			if b_key != my_key:
				send(('PREPARE', self.ballot_num), to=p)
			return
		if b_key != my_key:
			send(('PREPARE', self.ballot_num), to=p)
			return
		if b_key not in self.promises:
			self.promises[b_key] = {}
		self.promises[b_key][p] = set(acc_log)
		majority = (self.cluster_size // 2) + 1
		log_debug(f"{self.node_name} ACK counted: promises={len(self.promises[b_key])}, majority={majority}")
		if len(self.promises[b_key]) >= majority:
			become_leader()

	def receive(msg=('ACCEPT', b, seq, val), from_=leader):
		if self.is_failed:
			return
		if b < self.ballot_num:
			send(('NACK', (self.ballot_num[0], self.ballot_num[1])), to=leader)
			return
		self.ballot_num = b
		self.status = 'FOLLOWER'
		self.leader = leader
		self.current_leader = leader
		self.last_leader_msg = time.time()
		self.accept_log.add((b, seq, val))
		send(('ACCEPTED', b, seq, val, self), to=leader)

	def receive(msg=('NACK', b_key), from_=p):
		if self.status != 'LEADER':
			return
		my_key = (self.ballot_num[0], self.ballot_num[1])
		if b_key > my_key:
			self.status = 'FOLLOWER'
			self.last_leader_msg = time.time()

	def receive(msg=('ACCEPTED', b, seq, val, node), from_=_):
		if self.is_failed or self.status != 'LEADER':
			return
		key = (b, seq)
		voters = self.accepted_votes.setdefault(key, set())
		voters.add(node)
		if self not in voters:
			voters.add(self)
		needed = self.cluster_size / 2
		if len(voters) > needed:
			log_debug(f"{self.node_name} ACCEPTED majority reached: {len(voters)} > {needed} for seq={seq}")
			existing = self.committed_log.get(seq)
			if existing and is_prep(existing) and is_decision(val):
				handle_commit(b, seq, val)
			elif not existing:
				handle_commit(b, seq, val)
				send(('COMMIT', b, seq, val), to=self.live_peers)

	def receive(msg=('COMMIT', b, seq, val), from_=_):
		handle_commit(b, seq, val)

	def handle_commit(b, seq, val):
		existing = self.committed_log.get(seq)
		if existing:
			if is_prep(existing) and is_decision(val):
				# Apply the decision even if a prepare already committed at this slot.
				self.committed_log[seq] = val
				_, txid, decision, role = val
				apply_decision_entry(txid, decision, role)
			return
		self.committed_log[seq] = val
		if seq > self.next_seq:
			self.next_seq = seq + 1
		execute_committed()

	def execute_committed():
		while self.last_executed + 1 in self.committed_log:
			seq = self.last_executed + 1
			val = self.committed_log[seq]
			apply_entry(val)
			self.last_executed = seq

	# === Application logic ===
	def is_prep(val):
		tag = val[0] if isinstance(val, tuple) else None
		return tag in ('2PC_PREP', '2PC_PREP_ABORT')

	def is_decision(val):
		tag = val[0] if isinstance(val, tuple) else None
		return tag == '2PC_DECISION'

	def apply_entry(val):
		tag = val[0] if isinstance(val, tuple) else None
		if tag == 'LOCAL':
			_, tx = val
			apply_local(tx)
		elif tag == 'LOCAL_TX':
			_, txid, tx = val
			src, dst, amt = tx
			# Decide commit/abort deterministically at this log position.
			if self.db.get(src, INITIAL_BALANCE) < amt:
				reply = 'ABORT'
			else:
				apply_local(tx)
				reply = ('COMMIT', {src: self.db.get(src, INITIAL_BALANCE), dst: self.db.get(dst, INITIAL_BALANCE)})
			# Release any locks held for this txid (if any were used elsewhere)
			release_locks(txid)
			self.client_replies[txid] = reply
			client = self.tx_client.get(txid)
			if client:
				send(('TX_RESULT', txid, reply), to=client)
		elif tag == '2PC_PREP':
			_, txid, tx, role = val
			apply_prepare_entry(txid, tx, role, commit_allowed=True)
		elif tag == '2PC_PREP_ABORT':
			_, txid, tx, role = val
			apply_prepare_entry(txid, tx, role, commit_allowed=False)
		elif tag == '2PC_DECISION':
			_, txid, decision, role = val
			apply_decision_entry(txid, decision, role)
		elif tag == 'NO-OP':
			pass

	def apply_local(tx):
		src, dst, amt = tx
		if src not in self.db:
			self.db[src] = 10
		if dst not in self.db:
			self.db[dst] = 10
		if self.db[src] < amt:
			return
		self.db[src] -= amt
		self.db[dst] += amt
		self.modified_keys.add(src)
		self.modified_keys.add(dst)

	def apply_prepare_entry(txid, tx, role, commit_allowed:bool):
		src, dst, amt = tx
		meta = self.tx_meta.setdefault(txid, {'tx': tx, 'prepared': {}, 'decision': None, 'prepare_seq': None, 'commit_seq': None, 'client': None, 'participants': ()})
		if commit_allowed:
			if role == 'COORD':
				record_wal(src, txid)
				self.db[src] = self.db.get(src, 10) - amt
				self.modified_keys.add(src)
			elif role == 'PART':
				record_wal(dst, txid)
				self.db[dst] = self.db.get(dst, 10) + amt
				self.modified_keys.add(dst)
		meta['prepared'][role] = commit_allowed
		self.tx_meta[txid] = meta
		if role == 'PART':
			coord_shard = meta.get('coord_shard')
			if coord_shard:
				targets = self.cluster_map.get(coord_shard, set())
				if targets:
					log_debug(f"{self.node_name} apply_prepare_entry: sending PREPARED txid={txid} to coord_shard={coord_shard} ({len(targets)} targets)")
					part_write = {dst: self.db.get(dst, INITIAL_BALANCE)} if commit_allowed else {}
					send(('PREPARED', txid, tx, coord_shard, part_write), to=targets)
					# Store for retry if DECISION not received
					self.pending_prepared_outbox[txid] = {'coord_shard': coord_shard, 'tx': tx, 'part_write': part_write, 'last_sent': time.time(), 'decision_received': False, 'entry_ts': time.time()}
		if role == 'COORD' and self.status == 'LEADER' and meta['prepared'].get('PART') and not meta.get('decision'):
			decision = 'COMMIT'
			seq_c = meta.get('commit_seq') or meta.get('prepare_seq') or self.next_seq
			if seq_c is None:
				seq_c = self.next_seq
			propose_with_seq(seq_c, ('2PC_DECISION', txid, decision, 'COORD'))
			for shard in meta.get('participants', ()): 
				targets = self.cluster_map.get(shard, set())
				send(('DECISION', txid, decision), to=targets)
				self.decision_outbox[txid] = {'decision': decision, 'targets': set(targets), 'last_sent': time.time()}

	def apply_decision_entry(txid, decision, role):
		meta = self.tx_meta.get(txid)
		if not meta:
			return
		if meta.get('decision') and meta['decision'] == decision:
			return
		meta['decision'] = decision
		self.tx_meta[txid] = meta
		tx = meta.get('tx')
		if decision == 'COMMIT':
			release_locks(txid)
			if txid in self.wal:
				del self.wal[txid]
		else:
			undo_wal(txid)
			release_locks(txid)
		maybe_reply_client(txid, decision)
		if txid in self.pending_2pc:
			del self.pending_2pc[txid]
		if role == 'COORD' and txid in self.decision_outbox:
			del self.decision_outbox[txid]

	def cleanup_tx(txid):
		release_locks(txid)
		if txid in self.tx_table:
			del self.tx_table[txid]

	def maybe_reply_client(txid, decision):
		meta = self.tx_meta.get(txid)
		client = meta.get('client') if meta else None
		if client:
			reply = decision
			if decision == 'COMMIT' and meta and meta.get('tx'):
				tx = meta.get('tx')
				if isinstance(tx, tuple) and len(tx) >= 3:
					src, dst, _amt = tx[0], tx[1], tx[2]
					write_info = {src: self.db.get(src, INITIAL_BALANCE)}
					pw = meta.get('part_write') if isinstance(meta.get('part_write'), dict) else {}
					for k, v in (pw or {}).items():
						write_info[int(k)] = v
					reply = ('COMMIT', write_info)
			self.client_replies[txid] = reply
			send(('TX_RESULT', txid, reply), to=client)

	# === 2PC Handlers ===
	def receive(msg=('CLIENT_TX', tx, txid, client), from_=sender):
		if self.is_failed:
			return
		# Remember client for this txid so any replica that later commits it can reply (failover safe).
		if txid not in self.tx_client:
			self.tx_client[txid] = client
		log_debug(f"{self.node_name} receive(CLIENT_TX): txid={txid} tx={tx} status={self.status}")
		if self.status != 'LEADER':
			# Fast path: if we know a live leader, forward immediately (avoid client timeouts).
			if self.current_leader and self.current_leader != self and self.current_leader in self.live_peers:
				send(('CLIENT_TX', tx, txid, client), to=self.current_leader)
				return
			# If leader is unknown right after a failure, our local view may still include the failed old leader.
			# To avoid forwarding to a dead node, broadcast the request to all currently-live peers ONCE
			# (only when the sender is the client, not another node, to prevent ping-pong).
			if sender not in self.all_nodes and self.live_peers:
				send(('CLIENT_TX', tx, txid, client), to=self.live_peers)
			ensure_leader()
			if self.status != 'LEADER':
				# Buffer until leadership is known; leader will drain this queue on heartbeat loop.
				self.pending_requests.add((txid, tx, client))
				return
		handle_client_tx(tx, txid, client)

	def handle_client_tx(tx, txid, client):
		# Dedup/idempotence: if we already replied, resend the same reply.
		if txid in self.client_replies:
			send(('TX_RESULT', txid, self.client_replies[txid]), to=client)
			return
		
		# Extract consistency level (last element of tx tuple)
		consistency = 'linearizable'
		
		# Handle balance query: (key,) or (key, consistency)
		if len(tx) == 1:
			# Balance query without consistency: (key,) - linearizable
			cid = tx[0]
			bal = self.db.get(cid, INITIAL_BALANCE)
			send(('TX_RESULT', txid, ('BAL', bal)), to=client)
			self.tx_client[txid] = client
			return
		
		if len(tx) == 2:
			cid = tx[0]
			# Could be (key, consistency) or (src, dst) with missing amt
			if isinstance(tx[1], str):
				# Balance query with consistency: (key, consistency)
				consistency = tx[1]
				
				if consistency == 'eventual':
					# Eventual consistency: read from local replica immediately
					bal = self.db.get(cid, INITIAL_BALANCE)
					send(('TX_RESULT', txid, ('BAL', bal, 'eventual')), to=client)
					self.tx_client[txid] = client
					return
				elif consistency in ('ryw', 'read-your-writes'):
					# Read-your-writes: node returns normal read, Driver handles RYW cache
					bal = self.db.get(cid, INITIAL_BALANCE)
					send(('TX_RESULT', txid, ('BAL', bal, 'ryw')), to=client)
					self.tx_client[txid] = client
					return
				else:
					# Linearizable: normal read
					bal = self.db.get(cid, INITIAL_BALANCE)
					send(('TX_RESULT', txid, ('BAL', bal)), to=client)
					self.tx_client[txid] = client
					return
			else:
				# Invalid format (src, dst) without amount
				return
		
		if len(tx) == 4:
			# Transfer with consistency: (src, dst, amt, consistency)
			src, dst, amt, consistency = tx
		elif len(tx) == 3:
			# Transfer without explicit consistency
			src, dst, amt = tx[0], tx[1], tx[2]
			consistency = 'linearizable'
		else:
			return
		
		shard_src = shard_for(src)
		shard_dst = shard_for(dst)
		
		# For 'eventual' writes, still need consensus but can be more relaxed
		# For now, all writes go through normal path
		
		if shard_src == self.cluster_id and shard_dst == self.cluster_id:
			# Intra-shard transfers must be replicated via Paxos before replying (spec).
			# We replicate the request with its txid; commit/abort is decided at execution time
			# based on the replica state at that log position (deterministic across replicas).
			propose(('LOCAL_TX', txid, (src, dst, amt)))
			return
		participants = (shard_dst,)
		if not acquire_lock(src, txid):
			send(('TX_RESULT', txid, 'ABORT'), to=client)
			self.tx_client[txid] = client
			return
		sender_bal = self.db.get(src, 10)
		if sender_bal < amt:
			release_locks(txid)
			send(('TX_RESULT', txid, 'ABORT'), to=client)
			self.tx_client[txid] = client
			return
		seq_p = self.next_seq
		self.tx_meta[txid] = {'tx': (src, dst, amt), 'prepared': {}, 'decision': None, 'prepare_seq': seq_p, 'commit_seq': seq_p, 'client': client, 'participants': participants, 'ts': time.time(), 'consistency': consistency}
		self.tx_client[txid] = client
		propose_with_seq(seq_p, ('2PC_PREP', txid, (src, dst, amt), 'COORD'))
		targets = self.cluster_map.get(shard_dst, set())
		if targets:
			log_debug(f"{self.node_name} send PREPARE_TX txid={txid} to shard={shard_dst} targets={len(targets)}")
			send(('PREPARE_TX', txid, (src, dst, amt), self.cluster_id, seq_p, seq_p), to=targets)
		self.pending_2pc[txid] = {'state': 'WAIT_PREPARED', 'ts': time.time(), 'seq_p': seq_p, 'participants': set(participants), 'participants_ready': set(), 'last_prepare_sent': time.time()}

	def acquire_lock(key, txid):
		holder_info = self.locks.get(key)
		if holder_info:
			holder_txid, lock_ts = holder_info if isinstance(holder_info, tuple) else (holder_info, 0)
			if holder_txid and holder_txid != txid:
				# Check if lock is stale (held too long)
				now = time.time()
				if lock_ts and (now - lock_ts) > TWO_PC_TIMEOUT:
					# Stale lock - release it and continue
					log_debug(f"{self.node_name} acquire_lock: releasing stale lock on key={key} held by txid={holder_txid}")
					del self.locks[key]
				else:
					return False
		self.locks[key] = (txid, time.time())
		if key not in self.db:
			self.db[key] = 10
		return True

	def release_locks(txid):
		for k, v in list(self.locks.items()):
			holder_txid = v[0] if isinstance(v, tuple) else v
			if holder_txid == txid:
				del self.locks[k]

	def record_wal(key, txid):
		prev = self.db.get(key, 10)
		self.wal.setdefault(txid, []).append((key, prev))
		self.modified_keys.add(key)

	def undo_wal(txid):
		for key, prev in self.wal.get(txid, []):
			self.db[key] = prev
			self.modified_keys.add(key)
		if txid in self.wal:
			del self.wal[txid]

	def receive(msg=('PREPARE_TX', txid, tx, coord_shard, seq_p, seq_c), from_=leader):
		if self.is_failed:
			return
		if self.status != 'LEADER':
			log_debug(f"{self.node_name} PREPARE_TX txid={txid} deferred (status={self.status}, live_peers={len(self.live_peers)})")
			self.backlog_prepares.append((txid, tx, coord_shard, seq_p, seq_c))
			ensure_leader()
			return
		src, dst, amt = tx
		# Use participant's own seq instead of coordinator's seq_p to ensure sequential execution
		my_seq = self.next_seq
		log_debug(f"{self.node_name} receive PREPARE_TX txid={txid} from shard={coord_shard} using local seq={my_seq} live_cluster={self.live_cluster_size}")
		coord_targets = self.cluster_map.get(coord_shard, set())
		if not acquire_lock(dst, txid):
			log_debug(f"{self.node_name} PREPARE_TX txid={txid} lock failed for key={dst}, sending ABORT")
			propose(('2PC_PREP_ABORT', txid, tx, 'PART'))
			send(('ABORT_TX', txid), to=coord_targets)
			return
		self.tx_meta[txid] = {'tx': tx, 'prepared': {}, 'decision': None, 'prepare_seq': my_seq, 'commit_seq': my_seq, 'client': None, 'participants': (self.cluster_id,), 'ts': time.time(), 'coord_shard': coord_shard}
		log_debug(f"{self.node_name} PREPARE_TX txid={txid} calling propose seq={my_seq}")
		propose(('2PC_PREP', txid, tx, 'PART'))

	# Backward compatible (older form without part_write)
	def receive(msg=('PREPARED', txid, tx_payload, coord_hint), from_=p):
		receive(('PREPARED', txid, tx_payload, coord_hint, {}), from_=p)

	def receive(msg=('PREPARED', txid, tx_payload, coord_hint, part_write), from_=p):
		if self.is_failed:
			return
		# DistAlgo pattern matching already binds txid, tx_payload, coord_hint from the message
		log_debug(f"{self.node_name} receive(PREPARED): txid={txid} from {self.proc_to_name.get(p, str(p))} tx_known={tx_payload is not None}")
		meta = self.tx_meta.get(txid)
		if not meta:
			meta = {'tx': tx_payload, 'prepared': {}, 'decision': None, 'prepare_seq': None, 'commit_seq': None, 'client': None, 'participants': (), 'coord_shard': coord_hint, 'part_write': {}}
		if tx_payload and not meta.get('tx'):
			meta['tx'] = tx_payload
		if coord_hint and not meta.get('coord_shard'):
			meta['coord_shard'] = coord_hint
		# Store participant write info (dst balance) for RYW/response
		if isinstance(part_write, dict) and part_write:
			meta['part_write'] = dict(part_write)
		if meta.get('decision'):
			return
		meta.setdefault('prepared', {})['PART'] = True
		self.tx_meta[txid] = meta
		log_debug(f"{self.node_name} receive(PREPARED): after update, prepared={meta['prepared']}, status={self.status}")
		# If we are the coordinator shard leader but haven't yet marked COORD (e.g., PREPARED arrived before our own 2PC_PREP applied), fill it now once tx is known.
		if self.status == 'LEADER' and not meta['prepared'].get('COORD'):
			tx = meta.get('tx')
			coord_shard = meta.get('coord_shard')
			if tx and (coord_shard is None or coord_shard == self.cluster_id):
				apply_prepare_entry(txid, tx, 'COORD', commit_allowed=True)
		participants = meta.get('participants', ()) or ()
		if txid in self.pending_2pc and participants:
			self.pending_2pc[txid].setdefault('participants_ready', set()).add(next(iter(participants)))
		coord_shard = meta.get('coord_shard')
		# DO NOT unconditionally re-broadcast PREPARED - this causes message amplification
		# The PREPARED was already sent by apply_prepare_entry when the participant prepared
		if self.status != 'LEADER':
			fresh = self.current_leader if (self.current_leader and (time.time() - self.last_leader_msg) < HEARTBEAT_TIMEOUT) else None
			if not fresh:
				self.backlog_prepared.add(txid)
				ensure_leader()
			return
		if meta['prepared'].get('COORD') and not meta.get('decision'):
			decision = 'COMMIT'
			seq_c = meta.get('commit_seq') or meta.get('prepare_seq') or self.next_seq
			if seq_c is None:
				seq_c = self.next_seq
			propose_with_seq(seq_c, ('2PC_DECISION', txid, decision, 'COORD'))
			participant_shards = meta.get('participants', ())
			client = meta.get('client')
			for shard in participant_shards:
				targets = self.cluster_map.get(shard, set())
				send(('DECISION', txid, decision), to=targets)
				# For client RYW cache: include written balances when committing
				reply = decision
				if decision == 'COMMIT':
					tx = meta.get('tx')
					if tx and len(tx) >= 3:
						src, dst, _amt = tx[0], tx[1], tx[2]
						write_info = {src: self.db.get(src, INITIAL_BALANCE)}
						pw = meta.get('part_write') if isinstance(meta.get('part_write'), dict) else {}
						for k, v in (pw or {}).items():
							write_info[int(k)] = v
						reply = ('COMMIT', write_info)
				self.decision_outbox[txid] = {'decision': decision, 'client_reply': reply, 'targets': set(targets), 'last_sent': time.time(), 'client': client}
			if client:
				send(('TX_RESULT', txid, reply), to=client)

	def receive(msg=('ABORT_TX', txid), from_=p):
		if self.is_failed:
			return
		meta = self.tx_meta.get(txid)
		if not meta:
			return
		if self.status != 'LEADER':
			coord_targets = self.cluster_map.get(meta.get('coord_shard'), set()) if meta else set()
			if coord_targets:
				send(('ABORT_TX', txid), to=coord_targets)
			fresh = self.current_leader if (self.current_leader and (time.time() - self.last_leader_msg) < HEARTBEAT_TIMEOUT) else None
			if not fresh:
				self.backlog_decisions.append((txid, 'ABORT'))
				ensure_leader()
			return
		decision = 'ABORT'
		seq_c = meta.get('commit_seq') or meta.get('prepare_seq') or self.next_seq
		propose_with_seq(seq_c, ('2PC_DECISION', txid, decision, 'COORD'))
		participant_shards = meta.get('participants', ())
		client = meta.get('client')
		for shard in participant_shards:
			targets = self.cluster_map.get(shard, set())
			send(('DECISION', txid, decision), to=targets)
			self.decision_outbox[txid] = {'decision': decision, 'targets': set(targets), 'last_sent': time.time(), 'client': client}
		if client:
			send(('TX_RESULT', txid, decision), to=client)

	def receive(msg=('DECISION', txid, decision), from_=coord):
		if self.is_failed:
			return
		# Mark pending PREPARED as resolved (stop retrying)
		if txid in self.pending_prepared_outbox:
			self.pending_prepared_outbox[txid]['decision_received'] = True
		meta = self.tx_meta.get(txid)
		if not meta:
			# Create a stub so we can honor the decision and ack; avoids losing coord replies
			meta = {'tx': None, 'prepared': {}, 'decision': None, 'prepare_seq': None, 'commit_seq': None, 'client': None, 'participants': (), 'coord_shard': None}
			self.tx_meta[txid] = meta
		if meta.get('decision'):
			return
		if self.status != 'LEADER':
			self.backlog_decisions.append((txid, decision))
			ensure_leader()
			return
		# Use participant's own seq to ensure sequential execution
		propose(('2PC_DECISION', txid, decision, 'PART'))
		maybe_reply_client(txid, decision)
		send(('DECISION_ACK', txid), to=coord)

	def receive(msg=('DECISION_ACK', txid), from_=p):
		if self.is_failed:
			return
		if txid in self.decision_outbox:
			del self.decision_outbox[txid]

	# === Paxos propose ===
	def propose(val):
		seq = self.next_seq
		self.next_seq += 1
		self.accept_log.add((self.ballot_num, seq, val))
		send(('ACCEPT', self.ballot_num, seq, val), to=self.live_peers)
		send(('ACCEPTED', self.ballot_num, seq, val, self), to=self)

	def propose_with_seq(seq, val):
		if seq >= self.next_seq:
			self.next_seq = seq + 1
		self.accept_log.add((self.ballot_num, seq, val))
		send(('ACCEPT', self.ballot_num, seq, val), to=self.live_peers)
		send(('ACCEPTED', self.ballot_num, seq, val, self), to=self)

	# === Failure and control ===
	def receive(msg=('FAIL',)):
		# Fail-stop: stop participating in protocol and stop any election "timer" progress.
		# Force node out of CANDIDATE/LEADER so run-loop branches won't keep driving elections.
		self.status = 'FOLLOWER'
		self.is_failed = True
		self.last_leader_msg = time.time()
		self.last_prepare_received_time = time.time()
		for peer in self.live_peers:
			send(('PEER_FAILED', self), to=peer)

	def receive(msg=('RECOVER',)):
		self.is_failed = False
		self.last_leader_msg = time.time()
		for peer in self.live_peers:
			send(('PEER_RECOVERED', self), to=peer)
		
		# Optimization: If we are the designated initial leader, start election immediately
		# This reduces the leaderless window after RESET/RECOVER (e.g. at start of Set 5)
		if self == self.initial_leader_proc:
			start_election()

	def receive(msg=('RECOVER_WITH_PEERS', cluster_live_procs)):
		self.is_failed = False
		self.last_leader_msg = time.time()
		# Update live_peers to only include actually live nodes
		old_size = self.live_cluster_size
		self.live_peers = set(p for p in cluster_live_procs if p != self)
		self.live_cluster_size = len(self.live_peers) + 1
		log_debug(f"{self.node_name} RECOVER_WITH_PEERS: live_peers={len(self.live_peers)}, live_cluster_size={self.live_cluster_size} (was {old_size})")
		for peer in self.live_peers:
			send(('PEER_RECOVERED', self), to=peer)
		
		# Optimization: If we are the designated initial leader, start election immediately
		if self == self.initial_leader_proc:
			start_election()

	def receive(msg=('PEER_FAILED', p), from_=_):
		if self.is_failed:
			return
		peer_name = self.proc_to_name.get(p, str(p))
		if p in self.live_peers:
			self.live_peers.discard(p)
			self.live_cluster_size = self.cluster_size
			log_debug(f"{self.node_name} PEER_FAILED: {peer_name} removed, live_cluster_size now {self.live_cluster_size}")
			if p == self.current_leader and p != self:
				# Clear current_leader so ensure_leader() will trigger election
				self.current_leader = None
				self.leader = None
				log_debug(f"{self.node_name} leader {peer_name} failed, starting election")
				start_election()

	def receive(msg=('PEER_RECOVERED', p), from_=_):
		if self.is_failed:
			return
		if p not in self.live_peers and p != self:
			self.live_peers.add(p)
			self.live_cluster_size = self.cluster_size

	def receive(msg=('RESET',)):
		self.is_failed = False  # Critical: clear failure state on reset
		# Clear dynamic overrides on each node for per-set isolation (spec).
		SHARD_OVERRIDES.clear()
		self.accept_log = set()
		self.committed_log = {}
		self.accepted_votes = {}
		self.promises = {}
		self.next_seq = 1
		self.last_executed = 0
		self.db.reset()
		self.modified_keys = set()
		self.locks = {}
		self.tx_table = {}
		self.tx_meta = {}
		self.wal = {}
		self.decision_outbox = {}
		self.pending_2pc = {}
		self.tx_client = {}
		self.backlog_prepared = set()
		self.backlog_decisions = []
		self.backlog_prepares = []
		self.pending_prepared_outbox = {}
		self.new_view_messages = []
		self.reshard_events = []
		self.status = 'LEADER' if self.is_initial_leader else 'FOLLOWER'
		self.ballot_num = (1, self.node_name, self) if self.is_initial_leader else (0, self.node_name, self)
		self.leader = self if self.is_initial_leader else self.initial_leader_proc
		self.current_leader = self.leader
		# Note: live_peers will be updated by RECOVER_WITH_PEERS message
		self.live_peers = set(self.peers)
		self.live_cluster_size = self.cluster_size
		self.last_leader_msg = time.time()
		# Set grace period for initial leader to prevent early preemption
		if self.is_initial_leader:
			self.leader_grace_until = time.time() + ELECTION_TIMEOUT * 2
			self.new_view_messages.append(('NEW-VIEW', self.ballot_num, len(self.accept_log)))

	def receive(msg=('UPDATE_SHARD_OVERRIDES', overrides)):
		"""Update local dynamic shard mapping (offline resharding)."""
		if self.is_failed:
			return
		try:
			SHARD_OVERRIDES.clear()
			for k, sid in overrides.items():
				SHARD_OVERRIDES[int(k)] = int(sid)
		except Exception:
			# Best-effort; ignore malformed override maps.
			return

	def receive(msg=('UPDATE_SHARD_OVERRIDES_DELTA', delta)):
		"""Apply a delta update to local dynamic shard mapping (offline resharding)."""
		if self.is_failed:
			return
		try:
			if not isinstance(delta, dict):
				return
			for k, sid in delta.items():
				SHARD_OVERRIDES[int(k)] = int(sid)
		except Exception:
			return

	def receive(msg=('PRINT_DB', set_id)):
		# Failed nodes can still print their state before failure
		if not self.modified_keys:
			output(f"Set {set_id} {self.node_name} DB: []")
			return
		pairs = []
		for k in sorted(self.modified_keys):
			pairs.append(f"{k}:{self.db.get(k, INITIAL_BALANCE)}")
		changed = ", ".join(pairs)
		output(f"Set {set_id} {self.node_name} DB: {changed}")

	def receive(msg=('PRINT_VIEW', set_id)):
		# Failed nodes can still print their state before failure
		if not self.new_view_messages:
			output(f"Set {set_id} {self.node_name} View: []")
			return
		for tag, ballot, log_len in self.new_view_messages:
			b_round = ballot[0] if isinstance(ballot, tuple) and len(ballot) > 0 else ballot
			b_leader = ballot[1] if isinstance(ballot, tuple) and len(ballot) > 1 else ''
			output(f"Set {set_id} {self.node_name} View: {tag} {b_round} {b_leader} {log_len}")

	def receive(msg=('PRINT_BALANCE', key_id)):
		# Failed nodes can still print their state before failure
		bal = self.db.get(key_id, INITIAL_BALANCE)
		output(f"{self.node_name} : {bal}")

	def receive(msg=('GET_BALANCE', key_id, req_id, requester)):
		if self.is_failed:
			return
		bal = self.db.get(key_id, INITIAL_BALANCE)
		send(('BALANCE_RESULT', req_id, self.node_name, bal), to=requester)

	def receive(msg=('MIGRATE_KEY', key_id, dest_shard, dest_nodes)):
		"""Migrate a key from this shard to another shard."""
		if self.is_failed:
			return
		# Read current balance
		bal = self.db.get(key_id, INITIAL_BALANCE)
		# Record the migration event
		src_shard = self.cluster_id
		self.reshard_events.append((key_id, f"c{src_shard}", f"c{dest_shard}"))
		# Delete from local DB (optional: keep a tombstone)
		self.db[key_id] = 0  # Mark as migrated
		# Send to destination nodes
		send(('RECEIVE_KEY', key_id, bal, src_shard), to=dest_nodes)
		# Keep output clean for final submission; driver prints the required triplets.
		log_debug(f"{self.node_name} migrated key {key_id} (bal={bal}) to shard {dest_shard}")

	def receive(msg=('RECEIVE_KEY', key_id, balance, source_shard)):
		"""Receive a migrated key from another shard."""
		if self.is_failed:
			return
		# Insert key with balance into local DB
		self.db[key_id] = balance
		self.modified_keys.add(key_id)
		# Record the migration event
		self.reshard_events.append((key_id, f"c{source_shard}", f"c{self.cluster_id}"))
		log_debug(f"{self.node_name} received key {key_id} (bal={balance}) from shard {source_shard}")

	def receive(msg=('PRINT_RESHARD',)):
		# Failed nodes can still print their state before failure
		moves = getattr(self, 'reshard_events', []) or []
		output(f"{self.node_name} RESHARD: {moves}")

	def receive(msg=('STOP',)):
		self.shutdown = True


# === Parsing ===
def read_tests(csv_file):
	test_sets = []
	current = None
	with open(csv_file, 'r') as f:
		reader = csv.reader(f)
		next(reader, None)
		for row in reader:
			if not row or len(row) < 2:
				continue
			set_num = row[0].strip()
			tx_str = row[1].strip()
			live_nodes_str = row[2].strip() if len(row) > 2 else ''

			if set_num:
				if current:
					test_sets.append(current)
				current = {'id': set_num, 'transactions': [], 'live_nodes': parse_live_nodes(live_nodes_str)}

			if not current:
				continue

			if tx_str:
				if tx_str.startswith('F(') and tx_str.endswith(')'):
					current['transactions'].append(('FAIL', tx_str[2:-1]))
				elif tx_str.startswith('R(') and tx_str.endswith(')'):
					current['transactions'].append(('RECOVER', tx_str[2:-1]))
				else:
					tx = parse_transaction(tx_str)
					if tx:
						current['transactions'].append(('TX', tx))

		if current:
			test_sets.append(current)
	return test_sets


def main():
	import argparse
	parser = argparse.ArgumentParser(description='Distributed Transaction System')
	parser.add_argument('csv_file', nargs='?', default='CSE535-F25-Project-3-Testcases.csv',
						help='Test CSV file')
	parser.add_argument('--clusters', type=int, default=3,
						help='Number of clusters (default: 3)')
	parser.add_argument('--nodes-per-cluster', type=int, default=3,
						help='Nodes per cluster (default: 3)')
	parser.add_argument('--total-keys', type=int, default=9000,
						help='Total number of keys (default: 9000)')
	args = parser.parse_args()

	# Update configuration
	CLUSTER_CONFIG['num_clusters'] = args.clusters
	CLUSTER_CONFIG['nodes_per_cluster'] = args.nodes_per_cluster
	CLUSTER_CONFIG['total_keys'] = args.total_keys
	CLUSTER_CONFIG['shard_map'] = generate_shard_map(args.clusters, args.total_keys)

	num_clusters = args.clusters
	nodes_per_cluster = args.nodes_per_cluster
	total_nodes = num_clusters * nodes_per_cluster

	log_info(f"Using test file: {args.csv_file}")
	log_info(f"Configuration: {num_clusters} clusters, {nodes_per_cluster} nodes/cluster, {args.total_keys} keys")
	log_info(f"Shard map: {CLUSTER_CONFIG['shard_map']}")

	# Generate node names dynamically
	node_names = generate_nodes(num_clusters, nodes_per_cluster)
	nodes = new(Node, num=total_nodes)
	nodes_list = list(nodes)

	node_map = {name: proc for name, proc in zip(node_names, nodes_list)}

	# Build cluster_map dynamically
	cluster_map = {}
	for cid in range(1, num_clusters + 1):
		start_idx = (cid - 1) * nodes_per_cluster
		end_idx = start_idx + nodes_per_cluster
		cluster_map[cid] = set(nodes_list[start_idx:end_idx])

	# Leader for each cluster is the first node in that cluster
	leader_names = [node_names[(cid - 1) * nodes_per_cluster] for cid in range(1, num_clusters + 1)]
	leader_lookup = {name: node_map[name] for name in leader_names}
	cluster_leaders = {cid: node_map[leader_names[cid - 1]] for cid in range(1, num_clusters + 1)}

	log_info(f"Leaders: {leader_names}")

	# Create proc_to_name mapping
	proc_to_name = {proc: name for name, proc in node_map.items()}

	for i, node in enumerate(nodes_list):
		name = node_names[i]
		cid = (i // nodes_per_cluster) + 1
		peers = cluster_map[cid]
		initial_leader = (i % nodes_per_cluster) == 0  # First node in each cluster
		initial_leader_proc = cluster_leaders[cid]
		setup(node, (name, cid, peers, set(nodes_list), cluster_map, initial_leader, initial_leader_proc, proc_to_name))

	start(nodes)

	driver = new(Driver, num=1)
	setup(driver, (args.csv_file, node_map, cluster_map, cluster_leaders))
	start(driver)

	# Main returns; runtime will join child processes

